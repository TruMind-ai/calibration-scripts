{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env-db')\n",
    "from utils.utils import get_database, all_scales\n",
    "from transformers import LlamaTokenizerFast\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "import pandas as pd\n",
    "def count_tokens(prompt):\n",
    "    return len(tokenizer.tokenize(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy existing reference DB to new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_database()\n",
    "llms = ['OpenAssistant/llama2-13b-orca-8k-3319', 'openchat/openchat_v3.1', 'baichuan-inc/Baichuan-13B-Base']\n",
    "# llm_name = 'OpenAssistant/llama2-13b-orca-8k-3319'\n",
    "# llm_name = openchat/openchat_v3.1\n",
    "# llm_name = 'baichuan-inc/Baichuan-13B-Base'\n",
    "dim = 'contrast'\n",
    "# all_ratings_llm = pd.DataFrame(list(db[f'queries/{llm_name}/{dim}'].find({})))\n",
    "# all_ratings_llm = pd.DataFrame(list(db[f'queries/{dim}/{llm_name}'].find({})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### WARNING: THIS WILL REPLACE ALL QUERIES IN THE COLLECTION ####################\n",
    "############ for llm_name in llms:\n",
    "############     reference_queries = list(db[f'queries/reference/{dim}'].find({'tot_improved': False})) ##########\n",
    "############     # print(len(reference_queries))/\n",
    "############     db[f'queries/{llm_name}/{dim}'].insert_many(reference_queries) ###########\n",
    "# db.collection.create_index([(<key and index type specification>)], <options> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db[f'core_prompts/{dim}'].update_many({\"Prompt\": -100}, {\"$set\": {'rating': -1}})\n",
    "all_core_prompts = {}\n",
    "all_core_prompts_adjusted = {}\n",
    "for dim in all_scales:\n",
    "    if dim != 'contrast':\n",
    "        all_core_prompts[dim] = list(db[f'core_prompts/{dim}'].find({}))\n",
    "        all_core_prompts_adjusted[dim] = []\n",
    "        for prompt_doc in all_core_prompts[dim]:\n",
    "            if 'Prompt' in prompt_doc:\n",
    "                prompt_doc['combined_prompt'] = prompt_doc['Prompt']\n",
    "                prompt_doc.pop('Prompt')\n",
    "            else:\n",
    "                prompt_doc['tot_improved'] = True\n",
    "                prompt_doc.pop('dimension')\n",
    "            all_core_prompts_adjusted[dim].append(prompt_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in all_core_prompts_adjusted:\n",
    "    db[f'core_prompts/{dim}'].drop()\n",
    "    db[f'core_prompts/{dim}'].insert_many(all_core_prompts_adjusted[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_queries)\n",
    "import requests\n",
    "requests.post(\"http://localhost:8000/calibrations/cleanup\", data=\"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db[f'queries/{llm_name}/{dim}'].insert_many(all_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average latency\n",
    "latencies = list(db[f'queries/{llm_name}/{dim}'].find({\"latency\":{\"$gt\":0}}))\n",
    "latencies = pd.DataFrame(latencies)\n",
    "latencies['latency'].hist(bins=100)\n",
    "print(f\"Average latency: {sum(latencies['latency'])/len(latencies['latency'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies['rating'] = latencies['rating'].astype(int)\n",
    "latencies[(0 <= latencies['rating']) & (latencies['rating'] <= 10)]['rating'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[f'queries/{llm_name}/{dim}'].update_many({\"latency\":{\"$gt\":0}, \"rating\": -1}, {\"$set\": {\"latency\": -1, \"num_tries\": 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# # display dist. of prompt lengths\n",
    "# dim = 'contrast'\n",
    "# prefixes = {prefix['prefix_index']:prefix for prefix in list(db['prefixes'].find({}))}\n",
    "# prompts = {prompt['prompt_index']:prompt for prompt in list(db[f'core_prompts/{dim}'].find({}))}\n",
    "# samples = {sample['sample_index']:sample for sample in list(db[f'samples/{dim}'].find({}))}\n",
    "# tokens_dist = []\n",
    "# prefix = max(prefixes.values(), key=lambda x : len(x['prefix']))\n",
    "# for prompt in tqdm.tqdm(prompts):\n",
    "#     for sample in samples:\n",
    "#         t = count_tokens(prefix['prefix'] + prompts[prompt]['combined_prompt'] + samples[sample]['sample'])\n",
    "#         tokens_dist.append(t)\n",
    "# pd.Series(tokens_dist).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION contrast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION authority\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION consensus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION consistency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION scarcity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION unity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION liking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DIMENSION reciprocity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "skip_prefix_severe = ['reciprocity', 'unity', 'scarcity']\n",
    "for dim in all_scales:\n",
    "    query_docs = []\n",
    "    print(\"UPLOADING DIMENSION\", dim)\n",
    "    all_dim_prompts = list(db[f'core_prompts/{dim}'].find({}))\n",
    "    all_dim_samples = list(db[f'samples/{dim}'].find({}))\n",
    "    all_prefixes = list(db[f'prefixes'].find({}))\n",
    "    for prefix_doc in tqdm.tqdm(all_prefixes):\n",
    "        prefix_ind = prefix_doc['prefix_index']\n",
    "        if prefix_ind == 2 and dim in skip_prefix_severe:\n",
    "            continue\n",
    "        for prompt_doc in all_dim_prompts:\n",
    "            for sample_doc in all_dim_samples:\n",
    "                query_doc = {\n",
    "                    'prefix_index': prefix_doc['prefix_index'],\n",
    "                    'prompt_index': prompt_doc['prompt_index'],\n",
    "                    'sample_index': sample_doc['sample_index'],\n",
    "                    'tot_improved': False if 'tot_improved' in prompt_doc else True,\n",
    "                    'dimension': dim,\n",
    "                    'latency': -1.0,\n",
    "                    'num_tries': 0,\n",
    "                    'rating': -1,\n",
    "                }\n",
    "                if not query_doc['tot_improved']:\n",
    "                    query_docs.append(query_doc)\n",
    "    db[f'queries/reference/{dim}'].insert_many(query_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
